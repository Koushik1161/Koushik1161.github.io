---
layout: post
title: "AI Doc Pro: Minimal Document Analysis"
subtitle: "A focused PDF analyzer with GPT-4 summarization and entity extraction"
date: 2025-07-15
---

Sometimes the best tools do one thing well. AI Doc Pro is a document analyzer that extracts text from PDFs, generates summaries, identifies entities, and finds mathematical formulas. Fifty lines of Python backend, forty-seven lines of HTML frontend. That's it.

## The Core Loop

User drops a PDF. Frontend sends it to the `/analyze` endpoint. Backend extracts text, runs two concurrent GPT-4 calls (summary and entities), regex-matches math formulas, returns JSON. Frontend displays results with nice formatting.

No authentication. No database. No configuration UI. Just document in, analysis out.

## Text Extraction Trade-offs

PyPDF2 handles the extraction. It's not perfect—PDFs are notoriously inconsistent in how they encode text—but it works for most documents without requiring heavy dependencies like Tesseract or pdfminer.

The extracted text is truncated: 8000 characters for summarization, 6000 for entity extraction. This is a cost-performance trade-off. GPT-4 Turbo charges by tokens; sending entire documents gets expensive. The truncation assumes the important content is near the beginning, which is true for most structured documents.

## Concurrent API Calls

Summary and entity extraction are independent operations. Running them sequentially doubles latency unnecessarily. Python's `asyncio.gather` executes both calls concurrently, returning when both complete.

This pattern—identify independent operations, parallelize them—is fundamental to responsive AI applications. Users notice latency reduction even if they can't articulate why.

## The Entity Extraction Prompt

The NER prompt requests structured JSON output with specific categories:

- **PERSON**: Named individuals
- **ORG**: Organizations and companies
- **GPE**: Geopolitical entities (countries, cities)
- **DATE**: Temporal references
- **MONEY**: Financial amounts

GPT-4 is remarkably good at this structured extraction. The low temperature (0.2) ensures consistent, reliable outputs rather than creative interpretation.

## Math Formula Detection

Mathematical notation is extracted locally with regex, not sent to GPT-4. Patterns match both inline (`$...$`) and block (`$$...$$`) LaTeX-style formulas.

This is faster and cheaper than AI-based extraction. When pattern matching works, use it. Reserve AI for tasks that genuinely need reasoning.

The frontend renders detected formulas with KaTeX, displaying them beautifully regardless of how they appeared in the original PDF.

## The Minimal Frontend

Tailwind CSS with dark mode. Drag-and-drop file upload. Progress indicator during analysis. Results displayed with entity tags and formatted math.

No framework, no build process. A single HTML file with inline styles and vanilla JavaScript. It loads instantly and works everywhere.

The amber accent color gives it personality without complexity.

## What's Not Here

No user accounts. No saved analyses. No batch processing. No comparison features. No export options.

These features would be useful. They would also triple the codebase and complicate deployment. The current version solves a specific problem: "I have a PDF and want to understand what's in it quickly."

Feature creep is tempting. Resisting it keeps tools focused.

## Error Handling

The backend validates file types, returning HTTP 400 for non-PDFs. Text extraction failures are caught. API errors are logged. The system doesn't crash on unexpected input.

Minimal doesn't mean fragile. Basic error handling takes little code and prevents user frustration.

## Deployment Simplicity

FastAPI runs with `uvicorn main:app`. The frontend is a static file. No containers, no orchestration, no databases to configure.

This simplicity has trade-offs—no persistence, no scaling—but for a personal tool or prototype, it's exactly right.

## What I Learned

Fifty lines can do a lot when each line earns its place. AI APIs handle the heavy lifting; the application code is mostly plumbing.

Concurrency matters for perceived performance. Parallel API calls halve the wait time.

Cost constraints shape architecture. Truncating text isn't ideal but makes the tool economically viable for casual use.

Local processing (regex) and AI processing (GPT-4) complement each other. Use each where appropriate.

Minimal tools get used. Complex tools get abandoned. When a document needs quick analysis, a 100KB page that loads instantly wins over a full application that requires login and configuration.

Sometimes the best feature is the one you don't add.
