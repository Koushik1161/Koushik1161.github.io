---
layout: post
title: "SeeCal AI: Point, Shoot, Know What You're Eating"
subtitle: "Building an iOS app that turns food photos into nutritional insights"
date: 2025-05-08
---

Calorie tracking is tedious. You eat something, you open an app, you search through a database, you estimate portions, you log it manually. No wonder most people give up after a week.

What if you could just take a photo?

## The Vision: Camera-First Nutrition

SeeCal AI is an iOS app that uses computer vision to analyze food photos and estimate nutritional content. Point your camera at a meal, tap a button, get calories, protein, carbs, and fats. No searching, no manual entry, no friction.

## The Core Loop

The app flow is simple:

1. Open camera
2. Capture food photo
3. Wait 2-3 seconds
4. See nutritional breakdown
5. Optionally save to daily log

Behind that simplicity is a multimodal AI pipeline.

## GPT-4 Vision: The Heavy Lifting

I use OpenAI's GPT-4 Vision model to analyze food images:

```swift
func analyzeImage(_ image: UIImage) async throws -> NutritionResult {
    let base64Image = image.jpegData(compressionQuality: 0.8)?.base64EncodedString()

    let response = try await openai.chat.completions.create(
        model: "gpt-4-vision-preview",
        messages: [
            .user([
                .text("Analyze this food image. Estimate the nutritional content including calories, protein (g), carbohydrates (g), and fat (g). Return as JSON."),
                .image(base64Image, detail: .high)
            ])
        ]
    )

    return try parseNutritionJSON(response.content)
}
```

The key insight is that vision models are surprisingly good at food estimation. They can identify dishes, estimate portions, and provide reasonable nutritional approximations—not perfect, but good enough for daily tracking.

## Handling Model Uncertainty

AI estimates aren't gospel. I built in confidence scores and appropriate UI:

```swift
struct NutritionResult {
    let calories: Int
    let protein: Double
    let carbs: Double
    let fat: Double
    let confidence: Double
    let insights: String
}

// In the UI
if result.confidence < 0.6 {
    Text("Estimate may be less accurate for this dish")
        .font(.caption)
        .foregroundColor(.secondary)
}
```

When the model isn't confident, users know. This prevents false precision and sets appropriate expectations.

## The Orchestration Layer

I designed the architecture to support multiple AI services:

```swift
protocol AIServiceProtocol {
    func analyzeImage(_ image: UIImage) async throws -> NutritionResult
    func analyzeBarcode(_ code: String) async throws -> NutritionResult
    func analyzeText(_ description: String) async throws -> NutritionResult
}

class AIOrchestrationManager {
    let services: [AIServiceProtocol]

    func analyze(_ image: UIImage) async throws -> NutritionResult {
        // Run services in parallel
        return try await withTaskGroup(of: NutritionResult?.self) { group in
            for service in services {
                group.addTask {
                    try? await service.analyzeImage(image)
                }
            }

            // Collect and average results
            var results: [NutritionResult] = []
            for await result in group {
                if let r = result { results.append(r) }
            }

            return averageResults(results)
        }
    }
}
```

Currently I only use OpenAI, but the architecture is ready for adding Google's Gemini or other vision models. Running multiple models and averaging could improve accuracy.

## SwiftUI: Making It Feel Right

The UI uses SwiftUI's animation capabilities to make the experience feel alive:

```swift
struct NutritionCard: View {
    let value: Double
    let label: String
    let unit: String

    @State private var displayedValue: Double = 0

    var body: some View {
        VStack {
            Text("\(Int(displayedValue))")
                .font(.system(size: 36, weight: .bold))
                .contentTransition(.numericText())

            Text("\(label) (\(unit))")
                .font(.caption)
                .foregroundColor(.secondary)
        }
        .onAppear {
            withAnimation(.spring(duration: 0.8)) {
                displayedValue = value
            }
        }
    }
}
```

Numbers animate from zero to their final values. It's a small touch, but it makes results feel dynamic rather than static.

## Core Data: Keeping History

Meal logs persist locally using Core Data:

```swift
@Entity
class MealEntry {
    @Attribute var id: UUID
    @Attribute var date: Date
    @Attribute var imageData: Data?
    @Attribute var calories: Int
    @Attribute var protein: Double
    @Attribute var carbs: Double
    @Attribute var fat: Double
}
```

The daily view shows trends over time, helping users understand their eating patterns beyond individual meals.

## Lessons Learned

**Vision AI is good enough.** I expected food estimation to be a hard problem requiring specialized models. GPT-4 Vision handles it surprisingly well out of the box. Not perfect, but useful.

**Confidence matters more than precision.** Users don't need exact calorie counts—they need rough guidance. Communicating uncertainty is more important than chasing decimal places.

**Reduce friction relentlessly.** Every tap is a barrier. Every form field is friction. The camera-first approach works because it minimizes the steps between "I ate something" and "it's logged."

## What's Next

The obvious extension is barcode scanning for packaged foods. When a product has a barcode, we can look up exact nutritional info rather than estimating. The protocol-based architecture makes this easy to add.

I'm also interested in meal suggestions—if you've had a high-carb breakfast, what should you eat for dinner to balance your macros? The data's all there; it just needs synthesis.

---

*Built with SwiftUI, GPT-4 Vision, and the conviction that healthy eating shouldn't require spreadsheets.*
