---
layout: post
title: "Bently: Local-First Audio AI for Podcasters and Meeting Teams"
subtitle: "Building PodScribe and darkbee with Parakeet MLX for 30x faster transcription"
date: 2026-01-18
---

Cloud transcription services have a problem: they're slow, they're expensive, and they see everything you say. For podcasters discussing upcoming product launches or legal teams recording sensitive depositions, uploading audio to external servers isn't acceptable.

Bently is my exploration of local-first audio AI—two products (PodScribe for podcasters, darkbee for meeting teams) that keep audio on your Mac while delivering professional-quality transcription and AI-powered content generation.

## The Speed Breakthrough

The secret is Parakeet MLX, a speech-to-text model optimized for Apple Silicon. Benchmarks tell the story:

| Audio Length | Parakeet MLX | Whisper Large V3 |
|--------------|--------------|------------------|
| 10 minutes   | ~15 seconds  | ~2 minutes       |
| 30 minutes   | ~40 seconds  | ~6 minutes       |
| 60 minutes   | ~80 seconds  | ~12 minutes      |

That's 30x faster. A one-hour podcast episode transcribed while you grab coffee, not while you attend another meeting.

The speed comes from MLX, Apple's framework for machine learning on Metal. Parakeet's 600MB model (versus Whisper's 3GB+) loads faster and processes efficiently through the Neural Engine. Real-time factor of 3380x means it processes audio thousands of times faster than playback speed.

## PodScribe: Complete Podcast Content Suite

Transcription is just the beginning. PodScribe generates everything a podcast episode needs:

**Show Notes**: Bullet-pointed summaries with timestamps, ready for your podcast host.

**Blog Post Draft**: 300-400 word SEO-ready article, suitable for embedding on your website.

**Social Media Quotes**: Five tweetable extracts under 280 characters, pulled from the episode's most compelling moments.

**Episode Summary**: 2-3 sentence hook for podcast directories.

**Title Suggestions**: Three alternatives for A/B testing.

**SEO Tags**: 8-10 keywords for discoverability.

All generated by GPT-4o-mini after analyzing the transcript. The AI focuses on the first 15,000 characters to manage token costs while capturing the episode's key content.

## The Chunking Algorithm

Long podcasts require careful memory management. PodScribe automatically chunks audio files longer than 10 minutes into 5-minute segments:

```python
CHUNK_DURATION_SECONDS = 300  # 5 minutes
# Process each chunk separately
# Merge results with adjusted timestamps
# Clean up temp files
```

The sophisticated part: timestamps are recalculated so a phrase at 15:30 in the merged output reflects its actual position, not its position within its chunk. Users never see the chunking; they just see a coherent transcript.

## darkbee: Meeting Intelligence

While PodScribe targets podcasters, darkbee targets the Fireflies.ai market—meeting transcription with action item extraction.

The architecture splits real-time and post-processing:

**During meetings**: Recall.ai provides meeting bots for Zoom, Teams, Meet, and Webex. Deepgram Nova-3 powers live captions streamed via WebSocket.

**After meetings**: WhisperX with Pyannote handles speaker diarization—identifying who said what. Claude or GPT-4o generates summaries, extracts action items, and identifies key decisions.

The two-phase approach optimizes for different needs. Real-time captions prioritize speed. Post-processing prioritizes accuracy and insight.

## The Business Model Gap

Cloud services charge $8-40/month indefinitely. PodScribe offers $49 one-time pricing. The math is simple: buy once, own forever.

This works because local processing has near-zero marginal cost. After the initial purchase, users run the software on their own hardware. No server costs scale with usage.

For darkbee, the model shifts to $10-19/month SaaS—necessary because meeting bots and real-time APIs have ongoing costs. But unit economics still favor local processing: estimated $0.40-0.60 per meeting versus $2-5 for fully cloud-based competitors.

## Privacy as Feature

Neither product sends audio to external transcription APIs during processing. The flow:

1. Audio stays on your Mac
2. Parakeet MLX transcribes locally
3. Only the transcript text (optionally) goes to OpenAI for content generation

For users who want complete privacy, the AI features are optional. You can transcribe without any cloud calls.

This isn't paranoia—it's compliance. HIPAA for healthcare, legal privilege for attorneys, NDA protection for business development. Local processing makes the products usable where cloud services can't go.

## Dual Model Fallback

Robustness matters for production software. If Parakeet isn't installed:

```python
try:
    model = parakeet_load("mlx-community/parakeet-tdt-0.6b-v2")
    result = model.transcribe(file_path)
except ImportError:
    result = mlx_whisper.transcribe(file_path)
```

Users don't see error messages; they see slightly slower transcription. Graceful degradation over failure.

## What I Learned

**Local AI is genuinely competitive**. The quality gap between cloud and local has closed dramatically. Apple Silicon with MLX-optimized models delivers professional-grade results.

**Speed is a feature**. The difference between 80 seconds and 12 minutes isn't just convenience—it changes how people work. Fast enough for iterative workflows versus too slow to integrate.

**Privacy sells itself**. In regulated industries, local processing isn't a nice-to-have; it's a requirement. Products that respect this unlock markets that cloud-only competitors can't enter.

**One-time pricing works for local software**. Without ongoing server costs, perpetual licenses become viable. This positioning undercuts subscription competitors while delivering sustainable margins.

**The content generation bundle adds value**. Transcription is table stakes. Show notes, social quotes, and blog drafts are the features that save hours of manual work. The bundle justifies the price.

Bently proves that the future of audio AI isn't necessarily in the cloud. For users who care about speed, privacy, or economics, local-first processing offers compelling advantages.
