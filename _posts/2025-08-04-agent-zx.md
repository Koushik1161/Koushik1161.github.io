---
layout: post
title: "Agent ZX: Building an Autonomous Web Browser"
subtitle: "How I combined planning, execution, and reflection to create a self-correcting agent"
date: 2025-08-04
---

"Book a table for two at an Italian restaurant downtown for 8pm Friday."

That's a simple request for a human. For a computer, it's a nightmare of navigation, form-filling, decision-making, and error recovery. I built Agent ZX to handle exactly this kind of task.

## The Architecture: Four Components

Agent ZX splits the problem into distinct pieces:

**Planner**: Takes a goal and breaks it into steps. "Book a restaurant" becomes: 1) Search for Italian restaurants 2) Filter by availability 3) Select one with good reviews 4) Fill booking form 5) Confirm reservation.

**Executor**: Controls the browser. Takes screenshots, identifies interactive elements, clicks buttons, fills forms. This is where the rubber meets the road.

**Reflector**: Evaluates outcomes. Did that click work? Are we on the right page? Do we need to try a different approach?

**Memory**: Stores everything. Every screenshot, every action, every outcome. Enables replay and learning.

```python
class AgentZX:
    def __init__(self):
        self.planner = Planner(model="claude-3.5-sonnet")
        self.executor = Executor()
        self.reflector = Reflector(model="claude-3.5-sonnet")
        self.memory = ChromaMemory()

    async def run(self, goal: str):
        plan = self.planner.create_plan(goal)

        for step in plan.steps:
            result = await self.executor.execute(step)
            self.memory.store(step, result)

            reflection = self.reflector.evaluate(step, result)
            if reflection.needs_replan:
                plan = self.planner.replan(goal, self.memory.history)
```

The loop continues until the goal is achieved or the agent determines it's stuck.

## Vision-First Execution

Rather than parsing DOM trees (brittle, varies by site), the executor reasons about screenshots:

```python
async def execute(self, step: Step):
    # Capture current state
    screenshot = await self.browser.screenshot()

    # Label interactive elements with GPT-4o Vision
    labeled = await self.vision.label_elements(screenshot)

    # Decide action based on step goal and visible elements
    action = await self.vision.decide_action(
        step_goal=step.description,
        screenshot=labeled,
        history=self.recent_actions
    )

    # Perform action
    await self.browser.perform(action)

    # Capture result
    result_screenshot = await self.browser.screenshot()
    return ExecutionResult(before=screenshot, after=result_screenshot, action=action)
```

GPT-4o Vision looks at the labeled screenshot and decides: "Click the 'Reserve Now' button at coordinates (450, 320)." This works on any website, regardless of its HTML structure.

## Self-Correction Through Reflection

Things go wrong. The reflector catches problems:

```python
class Reflector:
    async def evaluate(self, step: Step, result: ExecutionResult):
        analysis = await self.model.analyze(
            prompt=f"""
            Step goal: {step.description}
            Action taken: {result.action}
            Before screenshot: {result.before}
            After screenshot: {result.after}

            Did the action achieve the step goal?
            Is the page in an expected state?
            Should we retry, continue, or replan?
            """
        )

        return Reflection(
            success=analysis.success,
            needs_retry=analysis.needs_retry,
            needs_replan=analysis.needs_replan,
            reasoning=analysis.reasoning
        )
```

If a click didn't work (page unchanged), try again. If we're on an unexpected page, replan from scratch. The agent adapts without human intervention.

## Dual-Model Strategy

I use different models for different purposes:

- **Claude 3.5 Sonnet** for planning and reflection. It reasons well about goals, steps, and what might go wrong.
- **GPT-4o** for vision tasks. It excels at understanding screenshots and identifying UI elements.

```python
# Planning uses Claude
planner = Planner(
    model=Anthropic("claude-3.5-sonnet"),
    system="You are a strategic planner breaking goals into actionable steps."
)

# Vision uses GPT-4o
vision = VisionExecutor(
    model=OpenAI("gpt-4o"),
    system="You analyze web screenshots and decide what to click/type."
)
```

Each model plays to its strengths.

## Full Transparency: Memory and Replay

Everything gets recorded:

```python
class ChromaMemory:
    def store(self, step, result, reflection):
        self.db.add(
            documents=[str(step)],
            metadatas=[{
                "action": result.action,
                "success": reflection.success,
                "timestamp": datetime.now().isoformat()
            }],
            images=[result.before, result.after]
        )

    def replay(self, task_id: str):
        history = self.db.get(where={"task_id": task_id})
        return Timeline(history)
```

Users can watch exactly what the agent did, step by step. This builds trust and enables debugging.

## The UI: Real-Time Observation

The frontend shows the agent working in real-time:

- Embedded VNC view of the browser
- Live timeline of actions and decisions
- Step-by-step progress through the plan
- Reflection notes explaining agent reasoning

Server-sent events stream updates as they happen:

```typescript
const eventSource = new EventSource('/api/agent/stream');

eventSource.onmessage = (event) => {
    const update = JSON.parse(event.data);
    updateTimeline(update);
    if (update.screenshot) updateBrowserView(update.screenshot);
};
```

You literally watch the agent navigate, click, and type.

## What I Learned

**Separation of concerns matters.** Planning, execution, reflection, and memory are different problems requiring different solutions. Keeping them separate makes each one better.

**Vision beats DOM parsing.** Websites change constantly. Screenshots don't lie. Reasoning about what you see is more robust than parsing what you hope the HTML looks like.

**Self-correction is essential.** Web automation breaks constantly. Agents that can detect and recover from failures are far more useful than ones that crash on the first obstacle.

---

*Built with Claude, GPT-4o, Playwright, and the conviction that agents should work on any website, not just the ones you planned for.*
