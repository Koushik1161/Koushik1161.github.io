---
layout: post
title: "Xelora: A Local-First Research Agent"
subtitle: "Building composable AI tools that stay on your machine"
date: 2025-04-24
---

There's a tension in AI tooling between power and privacy. Cloud services offer impressive capabilities, but your data leaves your machine. Local solutions keep data private, but often lack sophistication. With Xelora, I tried to find a middle ground.

## The Design Philosophy: Composable and Local

Xelora is a research agent that combines web search, news aggregation, content scraping, and AI analysis into a single pipeline. But unlike many agent frameworks, it's designed to be local-first and modular.

Each component is a standalone tool that can work independently:

```python
# Each tool is self-contained and testable
web_search = WebSearchTool()
scraper = WebScraperTool()
analyzer = ContentAnalyzerTool()
news = NewsAggregatorTool()

# Use individually or compose into a pipeline
results = web_search.search("quantum computing breakthroughs")
```

No complex orchestration framework required. Just Python functions that do one thing well.

## The Pipeline: Research as a Sequence

When you ask Xelora a research question, it executes a simple but effective sequence:

1. **Query Analysis**: Parse the question to understand what we're looking for
2. **Web Search**: Hit Google via SerpAPI for the top 3 results
3. **News Check**: Pull recent articles from NewsAPI for current context
4. **Content Extraction**: Scrape the search results for full text
5. **AI Analysis**: Use GPT-4o to summarize each source and extract key entities
6. **Packaging**: Bundle everything into structured JSON

```python
def research(query: str) -> dict:
    search_results = web_search.search(query, limit=3)
    news_results = news.fetch(query)

    analyses = []
    for result in search_results:
        content = scraper.extract(result.url)
        analysis = analyzer.analyze(content[:2000])  # Token budget
        analyses.append(analysis)

    return {
        "query": query,
        "sources": search_results,
        "news": news_results,
        "analyses": analyses
    }
```

Notice the `[:2000]` slice? That's intentional frugality. GPT-4o is powerful but not cheap. By capping content at 2000 characters per source, I keep costs low while still getting meaningful summaries.

## Smart Fallbacks

Real-world systems encounter errors. APIs fail, content blocks scrapers, rate limits kick in. Xelora handles this gracefully:

```python
def analyze(self, content: str) -> dict:
    try:
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": f"Analyze: {content}"}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except json.JSONDecodeError:
        # LLM didn't return valid JSON, fallback gracefully
        return {
            "summary": response.choices[0].message.content,
            "entities": []
        }
```

If JSON parsing fails, we don't crash—we return what we have. The summary is still useful even without structured entity extraction.

## The Interface: Streamlit for Rapid Iteration

I built the UI in Streamlit because it let me go from idea to working interface in an afternoon:

```python
st.title("Xelora Research Agent")
query = st.text_input("What would you like to research?")

if st.button("Research"):
    with st.spinner("Investigating..."):
        results = research(query)
    st.json(results)
```

Is it fancy? No. Does it work? Absolutely. And that JSON output is intentional—it makes Xelora useful not just for humans but as a component in larger systems.

## Docker-Ready from Day One

I containerized Xelora early because I wanted deployment to be trivial:

```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["streamlit", "run", "interface/streamlit_ui.py"]
```

Pull the image, set your API keys, run. No dependency hell, no environment conflicts.

## What I Learned

**Simplicity scales better than complexity.** Early versions had more sophisticated query analysis, multi-step research loops, all kinds of bells and whistles. I stripped most of it out. The simple linear pipeline handles 90% of use cases and is much easier to debug.

**Token budgets matter.** Without the 2000-character cap, a single research query could cost dollars in API fees. With it, costs stay under $0.10 per query. That's the difference between a toy and a tool you actually use.

**JSON output is a superpower.** By returning structured data instead of formatted text, Xelora can feed into other systems. Want to build a research aggregator that runs Xelora on multiple queries and synthesizes the results? The JSON output makes that trivial.

## The Bigger Picture

Xelora isn't trying to be the most powerful research agent. It's trying to be the most practical one—good enough for real work, cheap enough to use freely, simple enough to understand and modify.

In a world of increasingly complex AI frameworks, there's value in tools that do one thing well and stay out of your way.

---

*Built with Python, Streamlit, and a belief that the best tool is often the simplest one that gets the job done.*
